{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T10:30:21.371955Z",
     "start_time": "2025-03-17T10:30:10.684542Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import psycopg2\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T10:32:07.610521Z",
     "start_time": "2025-03-17T10:32:07.588333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0, x1, y1 = box\n",
    "    w, h = x1 - x0, y1 - y0\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ],
   "id": "b2c71396e74c3c65",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T10:32:19.777894Z",
     "start_time": "2025-03-17T10:32:09.464997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the SAM model and predictor\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# vit_b model checkpoint\n",
    "sam_checkpoint_vitb = \"../../experiments/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_vitb)\n",
    "sam.to(device=device)\n",
    "\n",
    "# Predictor for prompts\n",
    "predictor = SamPredictor(sam)"
   ],
   "id": "1a60d813aa5fcd9c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LarsBroertjes\\Documents\\GitHub\\thesis\\.venv\\lib\\site-packages\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T10:32:25.800617Z",
     "start_time": "2025-03-17T10:32:25.790165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# image folders\n",
    "Back = \"../../data/experimental/one_building_test/Back/\"\n",
    "Fwd = \"../../data/experimental/one_building_test/Fwd/\"\n",
    "Left = \"../../data/experimental/one_building_test/Left/\"\n",
    "Right = \"../../data/experimental/one_building_test/Right/\""
   ],
   "id": "9fe8d7998ae47221",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T10:32:35.427722Z",
     "start_time": "2025-03-17T10:32:35.414968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wkt_to_bbox(wkt):\n",
    "    coords = wkt.replace(\"POLYGON((\", \"\").replace(\"))\", \"\").split(\", \")\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    for coord in coords:\n",
    "        x, y = map(float, coord.split())\n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "    return [min(x_vals), min(y_vals), max(x_vals), max(y_vals)]\n",
    "\n",
    "def transform_bounding_boxes(boxes, img_width, img_height, direction):\n",
    "    transformed_boxes = []\n",
    "\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "\n",
    "        # Step 1: Rotate 90 degrees clockwise\n",
    "        new_x_min = y_min\n",
    "        new_y_min = img_width - x_max\n",
    "        new_x_max = y_max\n",
    "        new_y_max = img_width - x_min\n",
    "\n",
    "        # Step 2: Flip vertically\n",
    "        final_x_min = new_x_min\n",
    "        final_y_min = img_height - new_y_max\n",
    "        final_x_max = new_x_max\n",
    "        final_y_max = img_height - new_y_min\n",
    "\n",
    "        # Step 3: Shift downward (if boxes are too high)\n",
    "        shift_down = img_height / 3.1  # Shift down by half the image height\n",
    "        final_y_min += shift_down\n",
    "        final_y_max += shift_down\n",
    "\n",
    "        if direction == \"Fwd\":\n",
    "            transformed_boxes.append([final_x_min - 20, final_y_min - 300, final_x_max + 20, final_y_max + 20]) # Forward\n",
    "        elif direction == \"Left\":\n",
    "            transformed_boxes.append([final_x_min - 300, final_y_min - 100, final_x_max + 20, final_y_max + 200]) # Left\n",
    "        elif direction == \"Right\":\n",
    "            transformed_boxes.append([final_x_min - 20, final_y_min - 20, final_x_max + 300, final_y_max + 200]) # Right\n",
    "        elif direction == \"Back\":\n",
    "            transformed_boxes.append([final_x_min - 20, final_y_min - 20, final_x_max + 200, final_y_max + 600]) # Back\n",
    "\n",
    "    return transformed_boxes\n",
    "\n",
    "def get_unique_filename(output_dir, filename):\n",
    "    # Create a unique filename by appending a counter or timestamp if the file already exists\n",
    "    base_name, ext = os.path.splitext(filename)\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(output_dir, filename)):\n",
    "        filename = f\"{base_name}_{counter}{ext}\"\n",
    "        counter += 1\n",
    "    return filename\n",
    "\n",
    "def connect_to_database(imageid):\n",
    "    # Database configuration\n",
    "    DB_CONFIG = {\n",
    "        \"dbname\" : \"BagMapDB\",\n",
    "        \"user\" : \"postgres\",\n",
    "        \"password\" : os.getenv(\"DB_PASSWORD\"),\n",
    "        \"host\" : \"localhost\",\n",
    "        \"port\" : \"5432\"\n",
    "    }\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT bag_ids, bboxes FROM bag_in_image_utrecht WHERE image_name = %s;\", (imageid,))\n",
    "\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    if result:\n",
    "        bag_ids = result[0]  # List of bag IDs\n",
    "        bboxes_wkt = result[1]  # List of WKT polygons\n",
    "        return bag_ids, bboxes_wkt\n",
    "\n",
    "    return [], []"
   ],
   "id": "5170b366cb7c625d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T13:38:12.232086Z",
     "start_time": "2025-03-14T13:38:12.211600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MASK_OUTPUT_ROOT = \"../../data/masks/\"\n",
    "\n",
    "def segment(folder, imageid):\n",
    "    # Read the image and direction from folder\n",
    "    image_path = os.path.join(folder, imageid)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    direction = folder.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    # Extract bag_ids and their bboxes\n",
    "    bag_ids, bboxes_wkt = connect_to_database(imageid)\n",
    "\n",
    "    # Transform bounding boxes for SAM\n",
    "    bboxes = [wkt_to_bbox(wkt) for wkt in bboxes_wkt]\n",
    "    input_boxes = torch.tensor(bboxes, device=device)\n",
    "    input_boxes = transform_bounding_boxes(input_boxes.cpu().numpy(), image.shape[1], image.shape[0], direction)\n",
    "\n",
    "    buffer_size = 500  # Buffer size around input boxes\n",
    "\n",
    "    # Iterate over all input boxes\n",
    "    for i, prompt_box in enumerate(input_boxes):\n",
    "        if bag_ids[i] == \"0344100000157740\":\n",
    "            xmin, ymin, xmax, ymax = prompt_box\n",
    "\n",
    "            # Get the buffer coordinates\n",
    "            x_min_buff = max(0, int(xmin) - buffer_size)\n",
    "            y_min_buff = max(0, int(ymin) - buffer_size)\n",
    "            x_max_buff = min(image.shape[1], int(xmax) + buffer_size)\n",
    "            y_max_buff = min(image.shape[0], int(ymax) + buffer_size)\n",
    "\n",
    "            if x_min_buff >= x_max_buff or y_min_buff >= y_max_buff:\n",
    "                continue\n",
    "\n",
    "            # Crop image\n",
    "            cropped_image = image[y_min_buff:y_max_buff, x_min_buff:x_max_buff]\n",
    "\n",
    "            predictor.set_image(cropped_image)\n",
    "\n",
    "            # Define prompt box relative to cropped image\n",
    "            prompt_box_cropped_image = np.array([\n",
    "                max(0, min(cropped_image.shape[1] - 1, xmin - x_min_buff)),\n",
    "                max(0, min(cropped_image.shape[0] - 1, ymin - y_min_buff)),\n",
    "                max(0, min(cropped_image.shape[1] - 1, xmax - x_min_buff)),\n",
    "                max(0, min(cropped_image.shape[0] - 1, ymax - y_min_buff))\n",
    "            ], dtype=np.float32)\n",
    "\n",
    "            # Run segmentation\n",
    "            masks, scores, _ = predictor.predict(box=prompt_box_cropped_image, multimask_output=False)\n",
    "\n",
    "            if scores[0] > 0.0:\n",
    "                # Convert mask to binary (0 or 1)\n",
    "                binary_mask = (masks[0] > 0).astype(np.uint8)\n",
    "\n",
    "                # Create a full-size mask\n",
    "                full_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "                full_mask[y_min_buff:y_max_buff, x_min_buff:x_max_buff] = binary_mask\n",
    "\n",
    "                # Convert to 255 range for saving\n",
    "                full_mask = full_mask * 255\n",
    "\n",
    "                # Construct the corresponding mask path\n",
    "                relative_path = os.path.relpath(image_path, \"../../data/experimental/one_building_test/\")\n",
    "                mask_output_path = os.path.join(MASK_OUTPUT_ROOT, relative_path + \".png\")\n",
    "\n",
    "                # Create directory if not exists\n",
    "                os.makedirs(os.path.dirname(mask_output_path), exist_ok=True)\n",
    "\n",
    "                # Save binary mask\n",
    "                cv2.imwrite(mask_output_path, full_mask)\n",
    "\n",
    "\n"
   ],
   "id": "7c91da07cc60c7d3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T13:40:48.383194Z",
     "start_time": "2025-03-14T13:38:12.786986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for folder in [Back, Fwd, Left, Right]:\n",
    "    count = 0\n",
    "    for imageid in os.listdir(folder):\n",
    "        print(f\"Segmenting {folder} {imageid} {count}\")\n",
    "        count += 1\n",
    "        segment(folder, imageid)"
   ],
   "id": "54371b1ca099d786",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005059_0055_01_0074_P00_01.jpg 0\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005060_0055_01_0073_P00_01.jpg 1\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005061_0055_01_0072_P00_01.jpg 2\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005062_0055_01_0071_P00_01.jpg 3\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005063_0055_01_0070_P00_01.jpg 4\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005064_0055_01_0069_P00_01.jpg 5\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005065_0055_01_0068_P00_01.jpg 6\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005219_0056_01_0086_P00_01.jpg 7\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005220_0056_01_0087_P00_01.jpg 8\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005221_0056_01_0088_P00_01.jpg 9\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005222_0056_01_0089_P00_01.jpg 10\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005223_0056_01_0090_P00_01.jpg 11\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005224_0056_01_0091_P00_01.jpg 12\n",
      "Segmenting ../../data/experimental/one_building_test/Back/ 262005225_0056_01_0092_P00_01.jpg 13\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005041_0055_01_0092_P00_01.jpg 0\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005201_0056_01_0068_P00_01.jpg 1\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005202_0056_01_0069_P00_01.jpg 2\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005203_0056_01_0070_P00_01.jpg 3\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005204_0056_01_0071_P00_01.jpg 4\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005205_0056_01_0072_P00_01.jpg 5\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005206_0056_01_0073_P00_01.jpg 6\n",
      "Segmenting ../../data/experimental/one_building_test/Fwd/ 201005207_0056_01_0074_P00_01.jpg 7\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231003980_0052_01_0074_P00_01.jpg 0\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231003981_0052_01_0075_P00_01.jpg 1\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231003982_0052_01_0076_P00_01.jpg 2\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231003983_0052_01_0077_P00_01.jpg 3\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231006289_0059_01_0101_P00_01.jpg 4\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231006290_0059_01_0100_P00_01.jpg 5\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231006291_0059_01_0099_P00_01.jpg 6\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231006292_0059_01_0098_P00_01.jpg 7\n",
      "Segmenting ../../data/experimental/one_building_test/Left/ 231006293_0059_01_0097_P00_01.jpg 8\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183003833_0051_01_0073_P00_01.jpg 0\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183004438_0053_01_0079_P00_01.jpg 1\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183004439_0053_01_0078_P00_01.jpg 2\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183004440_0053_01_0077_P00_01.jpg 3\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183004441_0053_01_0076_P00_01.jpg 4\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183004442_0053_01_0075_P00_01.jpg 5\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183005840_0058_01_0090_P00_01.jpg 6\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183005841_0058_01_0091_P00_01.jpg 7\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183005842_0058_01_0092_P00_01.jpg 8\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183005843_0058_01_0093_P00_01.jpg 9\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183005844_0058_01_0094_P00_01.jpg 10\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183006493_0060_01_0103_P00_01.jpg 11\n",
      "Segmenting ../../data/experimental/one_building_test/Right/ 183006494_0060_01_0104_P00_01.jpg 12\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b22ae93ea073f2ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
